{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\oscar\\anaconda3\\envs\\cvpr\\lib\\site-packages\\torch_geometric\\typing.py:72: UserWarning: An issue occurred while importing 'torch-scatter'. Disabling its usage. Stacktrace: [WinError 127] The specified procedure could not be found\n",
      "  warnings.warn(f\"An issue occurred while importing 'torch-scatter'. \"\n",
      "c:\\Users\\oscar\\anaconda3\\envs\\cvpr\\lib\\site-packages\\torch_geometric\\typing.py:99: UserWarning: An issue occurred while importing 'torch-spline-conv'. Disabling its usage. Stacktrace: [WinError 127] The specified procedure could not be found\n",
      "  warnings.warn(\n",
      "c:\\Users\\oscar\\anaconda3\\envs\\cvpr\\lib\\site-packages\\torch_geometric\\typing.py:110: UserWarning: An issue occurred while importing 'torch-sparse'. Disabling its usage. Stacktrace: [WinError 127] The specified procedure could not be found\n",
      "  warnings.warn(f\"An issue occurred while importing 'torch-sparse'. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import laspy\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "from scipy.spatial import ConvexHull\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from mpl_toolkits.mplot3d.art3d import Poly3DCollection\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch_geometric.data import Data\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import knn_graph, MessagePassing\n",
    "from torch_geometric.utils import softmax\n",
    "\n",
    "import context\n",
    "import utils \n",
    "import utils_ChallengeBuilding3D as ut_CB3D\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DEFINE FOLDERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base folder:  c:\\Users\\oscar\\OneDrive - Fondazione Bruno Kessler\\Building_3D_Challenge\\script\n",
      "Folder:  c:\\Users\\oscar\\OneDrive - Fondazione Bruno Kessler\\Building_3D_Challenge\\data\n",
      "Output folder: c:\\Users\\oscar\\OneDrive - Fondazione Bruno Kessler\\Building_3D_Challenge\\outputs\n"
     ]
    }
   ],
   "source": [
    "base_folder = Path(os.getcwd())\n",
    "folder = base_folder.parents[0] / 'data'\n",
    "output_folder = base_folder.parents[0] / 'outputs'\n",
    "\n",
    "print ('Base folder: ', base_folder)\n",
    "print ('Folder: ', folder)\n",
    "print ('Output folder:', output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted .xyz to .las files: ['c:\\\\Users\\\\oscar\\\\OneDrive - Fondazione Bruno Kessler\\\\Building_3D_Challenge\\\\data\\\\10052.las', 'c:\\\\Users\\\\oscar\\\\OneDrive - Fondazione Bruno Kessler\\\\Building_3D_Challenge\\\\data\\\\9960.las', 'c:\\\\Users\\\\oscar\\\\OneDrive - Fondazione Bruno Kessler\\\\Building_3D_Challenge\\\\data\\\\9961.las', 'c:\\\\Users\\\\oscar\\\\OneDrive - Fondazione Bruno Kessler\\\\Building_3D_Challenge\\\\data\\\\9962.las', 'c:\\\\Users\\\\oscar\\\\OneDrive - Fondazione Bruno Kessler\\\\Building_3D_Challenge\\\\data\\\\9963.las', 'c:\\\\Users\\\\oscar\\\\OneDrive - Fondazione Bruno Kessler\\\\Building_3D_Challenge\\\\data\\\\9964.las', 'c:\\\\Users\\\\oscar\\\\OneDrive - Fondazione Bruno Kessler\\\\Building_3D_Challenge\\\\data\\\\9965.las', 'c:\\\\Users\\\\oscar\\\\OneDrive - Fondazione Bruno Kessler\\\\Building_3D_Challenge\\\\data\\\\9966.las', 'c:\\\\Users\\\\oscar\\\\OneDrive - Fondazione Bruno Kessler\\\\Building_3D_Challenge\\\\data\\\\9967.las', 'c:\\\\Users\\\\oscar\\\\OneDrive - Fondazione Bruno Kessler\\\\Building_3D_Challenge\\\\data\\\\9968.las', 'c:\\\\Users\\\\oscar\\\\OneDrive - Fondazione Bruno Kessler\\\\Building_3D_Challenge\\\\data\\\\9969.las', 'c:\\\\Users\\\\oscar\\\\OneDrive - Fondazione Bruno Kessler\\\\Building_3D_Challenge\\\\data\\\\997.las', 'c:\\\\Users\\\\oscar\\\\OneDrive - Fondazione Bruno Kessler\\\\Building_3D_Challenge\\\\data\\\\9971.las', 'c:\\\\Users\\\\oscar\\\\OneDrive - Fondazione Bruno Kessler\\\\Building_3D_Challenge\\\\data\\\\9972.las', 'c:\\\\Users\\\\oscar\\\\OneDrive - Fondazione Bruno Kessler\\\\Building_3D_Challenge\\\\data\\\\9973.las', 'c:\\\\Users\\\\oscar\\\\OneDrive - Fondazione Bruno Kessler\\\\Building_3D_Challenge\\\\data\\\\9974.las', 'c:\\\\Users\\\\oscar\\\\OneDrive - Fondazione Bruno Kessler\\\\Building_3D_Challenge\\\\data\\\\9975.las', 'c:\\\\Users\\\\oscar\\\\OneDrive - Fondazione Bruno Kessler\\\\Building_3D_Challenge\\\\data\\\\9976.las', 'c:\\\\Users\\\\oscar\\\\OneDrive - Fondazione Bruno Kessler\\\\Building_3D_Challenge\\\\data\\\\9977.las', 'c:\\\\Users\\\\oscar\\\\OneDrive - Fondazione Bruno Kessler\\\\Building_3D_Challenge\\\\data\\\\9978.las', 'c:\\\\Users\\\\oscar\\\\OneDrive - Fondazione Bruno Kessler\\\\Building_3D_Challenge\\\\data\\\\9979.las', 'c:\\\\Users\\\\oscar\\\\OneDrive - Fondazione Bruno Kessler\\\\Building_3D_Challenge\\\\data\\\\9980.las', 'c:\\\\Users\\\\oscar\\\\OneDrive - Fondazione Bruno Kessler\\\\Building_3D_Challenge\\\\data\\\\9981.las', 'c:\\\\Users\\\\oscar\\\\OneDrive - Fondazione Bruno Kessler\\\\Building_3D_Challenge\\\\data\\\\999.las', 'c:\\\\Users\\\\oscar\\\\OneDrive - Fondazione Bruno Kessler\\\\Building_3D_Challenge\\\\data\\\\9990.laz.las', 'c:\\\\Users\\\\oscar\\\\OneDrive - Fondazione Bruno Kessler\\\\Building_3D_Challenge\\\\data\\\\9991.laz.las', 'c:\\\\Users\\\\oscar\\\\OneDrive - Fondazione Bruno Kessler\\\\Building_3D_Challenge\\\\data\\\\9993.laz.las', 'c:\\\\Users\\\\oscar\\\\OneDrive - Fondazione Bruno Kessler\\\\Building_3D_Challenge\\\\data\\\\9994.laz.las', 'c:\\\\Users\\\\oscar\\\\OneDrive - Fondazione Bruno Kessler\\\\Building_3D_Challenge\\\\data\\\\9996.las', 'c:\\\\Users\\\\oscar\\\\OneDrive - Fondazione Bruno Kessler\\\\Building_3D_Challenge\\\\data\\\\9997.las', 'c:\\\\Users\\\\oscar\\\\OneDrive - Fondazione Bruno Kessler\\\\Building_3D_Challenge\\\\data\\\\9998.las', 'c:\\\\Users\\\\oscar\\\\OneDrive - Fondazione Bruno Kessler\\\\Building_3D_Challenge\\\\data\\\\9999.las']. Type of las_files: <class 'list'>\n"
     ]
    }
   ],
   "source": [
    "las_files = ut_CB3D.xyz_to_las(folder)\n",
    "print(f'Converted .xyz to .las files: {las_files}. Type of las_files: {type(las_files)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "points_dict = ut_CB3D.process_las_files(folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points_dict.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize dictionaries to store the results for each file\n",
    "all_points = {}\n",
    "all_center_mass = {}\n",
    "all_shifted_points = {}\n",
    "all_cov_matrix = {}\n",
    "\n",
    "# Process each set of 3D points in points_dict\n",
    "for file_name, data in points_dict.items():\n",
    "\n",
    "    # Retrieve original points\n",
    "    points = data['points_3d']\n",
    "    print(f'File name: {file_name}\\nOriginal points:\\n{points[:5]}... (showing first 5 of {len(points)})')\n",
    "\n",
    "    # Calculate center of mass and shift points\n",
    "    center_mass, shifted_points = ut_CB3D.center_data(points)\n",
    "    print(f'Center of Mass: {center_mass}')\n",
    "    print(f'Shifted points:\\n{shifted_points[:5]}... (showing first 5 of {len(shifted_points)})')\n",
    "\n",
    "    # Compute covariance matrix of the shifted points\n",
    "    cov_matrix = ut_CB3D.compute_covariance_matrix(shifted_points)\n",
    "    print(f'Covariance matrix:\\n{cov_matrix}')\n",
    "\n",
    "    # Register points back by adding the center of mass\n",
    "    registered_points = ut_CB3D.recenter_data(shifted_points, center_mass)\n",
    "    print(f'Registered points:\\n{registered_points[:5]}... (showing first 5 of {len(registered_points)})')\n",
    "\n",
    "    # Verify if registered points are equal to the original points\n",
    "    assert np.allclose(points, registered_points), \"Registered points do not match the original points.\"\n",
    "\n",
    "    # Store results in the respective dictionaries\n",
    "    all_points[file_name] = registered_points\n",
    "    all_center_mass[file_name] = center_mass\n",
    "    all_shifted_points[file_name] = shifted_points\n",
    "    all_cov_matrix[file_name] = cov_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assert np.allclose(points, registered_points), \"Registered points do not match the original points.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition_and_compute_covariance(closest_points, n):\n",
    "\n",
    "    dot_products = np.dot(closest_points, n)\n",
    "\n",
    "    N_i_upper = closest_points[dot_products >= 0]\n",
    "    N_i_lower = closest_points[dot_products < 0]\n",
    "\n",
    "    # Check if either partition is empty\n",
    "    if N_i_upper.size == 0 or N_i_lower.size == 0:\n",
    "        \n",
    "        print(\"Empty partition detected.\")\n",
    "        \n",
    "        return None, None, None, None, None, None\n",
    "\n",
    "    # Compute covariance matrices for both partitions\n",
    "    K_i_upper = np.cov(N_i_upper, rowvar=False)\n",
    "    K_i_lower = np.cov(N_i_lower, rowvar=False)\n",
    "\n",
    "    # Perform SVD on the covariance matrices\n",
    "    _, Sigma_i_upper, _ = np.linalg.svd(K_i_upper)\n",
    "    _, Sigma_i_lower, _ = np.linalg.svd(K_i_lower)\n",
    "\n",
    "    # Compute the centers of mass for both partitions\n",
    "    N_i_upper_center = np.mean(N_i_upper, axis=0)\n",
    "    N_i_lower_center = np.mean(N_i_lower, axis=0)\n",
    "\n",
    "    return N_i_upper, N_i_lower, Sigma_i_upper, Sigma_i_lower, N_i_upper_center, N_i_lower_center\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty partition detected.\n",
      "Empty partition detected.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the results list to collect data\n",
    "results = []\n",
    "\n",
    "# Process each set of 3D points in points_dict\n",
    "for file_name, data in points_dict.items():\n",
    "    points = data['points_3d']  # Get the 3D points from the dictionary\n",
    "\n",
    "    # Compute the covariance matrix for the entire set of points\n",
    "    cov_matrix = ut_CB3D.compute_covariance_matrix(points)\n",
    "\n",
    "    # Perform SVD\n",
    "    U, singular_values, Vt = ut_CB3D.perform_svd(cov_matrix)\n",
    "\n",
    "    # Check if SVD was successful\n",
    "    if U is None or singular_values is None or Vt is None:\n",
    "        print(f\"SVD failed for file: {file_name}\")\n",
    "        continue\n",
    "\n",
    "    # Center the points by subtracting the mean\n",
    "    mean, shifted_points = ut_CB3D.center_data(points)\n",
    "\n",
    "    # Normalize the shifted points using the singular values and U\n",
    "    normalized_points = ut_CB3D.normalize_neighborhood(shifted_points, singular_values, U)\n",
    "\n",
    "    # Extract eigenvector of the smallest singular value and closest points\n",
    "    n, closest_points = ut_CB3D.extract_eigenvector_and_neighbors(normalized_points, Vt)\n",
    "\n",
    "    # Values for unpacking edges (assuming partition_and_compute_covariance is defined elsewhere)\n",
    "    N_i_upper, N_i_lower, Sigma_i_upper, Sigma_i_lower, N_i_upper_center, N_i_lower_center = partition_and_compute_covariance(closest_points, n)\n",
    "\n",
    "    # Calculate perpendicular and tangential distances using the closest points, mean, and eigenvector n\n",
    "    s_perpendicular, s_tangential = ut_CB3D.calculate_distances(closest_points, mean, n)\n",
    "\n",
    "    # Store the results or perform additional processing\n",
    "    results.append({\n",
    "        'file_name': file_name,\n",
    "        'mean': mean,\n",
    "        'shifted_points': shifted_points,\n",
    "        'normalized_points': normalized_points,\n",
    "        'points': points,\n",
    "        'n': n,\n",
    "        'closest_points': closest_points,\n",
    "        'N_i_upper': N_i_upper,\n",
    "        'N_i_lower': N_i_lower,\n",
    "        'Sigma_i_upper': Sigma_i_upper,\n",
    "        'Sigma_i_lower': Sigma_i_lower,\n",
    "        'N_i_upper_center': N_i_upper_center,\n",
    "        'N_i_lower_center': N_i_lower_center,\n",
    "        's_perpendicular': s_perpendicular,\n",
    "        's_tangential': s_tangential\n",
    "    })\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DEEP LEARNING APPROACH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 3 # k >> include more points in the neighborhood, k << focus on a more local neighborhood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PCEDNet(nn.Module):\n",
    "    def __init__(self, input_dim=12, hidden_dim=64, output_dim=2, radius=0.7):\n",
    "        \n",
    "        super(PCEDNet, self).__init__()\n",
    "        self.radius = radius  # Radius for edge detection\n",
    "\n",
    "        # MLP layers to embed features\n",
    "        self.mlp1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.mlp2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.mlp3 = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "        # Batch Normalization\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_dim)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_dim)\n",
    "        self.bn3 = nn.BatchNorm1d(hidden_dim)\n",
    "\n",
    "        # Attention layer: computes attention scores for neighbor features\n",
    "        self.attention = nn.Linear(2 * hidden_dim, 1)\n",
    "\n",
    "        # Classification layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "        # Activation\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, points, features):\n",
    "        B, N, _ = points.shape\n",
    "\n",
    "        # Step 1: Embed the input features using MLP layers\n",
    "        x = self.mlp1(features)                       # (B, N, hidden_dim)\n",
    "        x = x.view(B * N, -1)                         # Reshape to (B*N, hidden_dim) for BatchNorm1d\n",
    "        x = self.relu(self.bn1(x))                    # (B*N, hidden_dim)\n",
    "\n",
    "        x = self.mlp2(x)                              # (B*N, hidden_dim)\n",
    "        x = self.relu(self.bn2(x))                    # (B*N, hidden_dim)\n",
    "\n",
    "        x = self.mlp3(x)                              # (B*N, hidden_dim)\n",
    "        x = self.relu(self.bn3(x))                    # (B*N, hidden_dim)\n",
    "\n",
    "        x = x.view(B, N, -1)                          # Reshape back to (B, N, hidden_dim)\n",
    "\n",
    "        # Step 2: Compute radius graph\n",
    "        points_flat = points.view(B * N, 3)  # Flatten points to (B*N, 3)\n",
    "        edge_index = radius_graph(points_flat, 0.70)  # (2, E)\n",
    "\n",
    "        #  edge_index = radius_graph(points_flat, r=radius)  # (2, E)\n",
    "\n",
    "        # Step 3: Message Passing with Attention\n",
    "        source = x.view(B * N, -1)[edge_index[0]]     # (E, hidden_dim)\n",
    "        target = x.view(B * N, -1)[edge_index[1]]     # (E, hidden_dim)\n",
    "\n",
    "        concat = torch.cat([source, target], dim=1)   # (E, 2*hidden_dim)\n",
    "        attn_scores = self.attention(concat)          # (E, 1)\n",
    "        attn_scores = F.leaky_relu(attn_scores)\n",
    "        attn_scores = F.softmax(attn_scores, dim=0)  # Apply softmax over the edge dimension\n",
    "\n",
    "        weighted = source * attn_scores               # (E, hidden_dim)\n",
    "\n",
    "        # Aggregate weighted messages back to the nodes\n",
    "        agg = torch.zeros_like(x.view(B * N, -1))     # (B*N, hidden_dim)\n",
    "        agg = agg.index_add(0, edge_index[0], weighted)\n",
    "        agg = agg.view(B, N, -1)                      # (B, N, hidden_dim)\n",
    "\n",
    "        # Step 4: Classify each point based on the aggregated features\n",
    "        logits = self.fc(agg)                         # (B, N, output_dim)\n",
    "        edge_probabilities = F.log_softmax(logits, dim=-1)  # (B, N, output_dim)\n",
    "\n",
    "        return edge_probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_scale_feature(points_3d, k):\n",
    "    \n",
    "    n_points = points_3d.shape[0]\n",
    "    features = np.zeros((n_points, 12))\n",
    "\n",
    "    # Use NearestNeighbors to find the k nearest neighbors for each point\n",
    "    nbrs = NearestNeighbors(n_neighbors=k, algorithm='auto').fit(points_3d)\n",
    "    distances, indices = nbrs.kneighbors(points_3d)\n",
    "    \n",
    "    # Calculate the center of mass for each neighborhood\n",
    "    N_i_k = np.mean(points_3d[indices], axis=1)  # Center of mass for scale k\n",
    "    \n",
    "    # Set k0 to the minimum of k0 or the number of points\n",
    "    k0 = min(max(k, 128), n_points)  # Ensure k0 does not exceed the number of points\n",
    "    nbrs_k0 = NearestNeighbors(n_neighbors=k0, algorithm='auto').fit(points_3d)\n",
    "    _, indices_k0 = nbrs_k0.kneighbors(points_3d)\n",
    "    N_i_k0 = np.mean(points_3d[indices_k0], axis=1)  # Center of mass for scale k0\n",
    "\n",
    "    # Calculate tangential and perpendicular distances\n",
    "    for i in range(n_points):\n",
    "        delta_N = N_i_k[i] - (N_i_k0[i] - points_3d[i])\n",
    "        ni_k0 = (N_i_k0[i] - points_3d[i]) / np.linalg.norm(N_i_k0[i] - points_3d[i])\n",
    "        \n",
    "        # Perpendicular component\n",
    "        c_i_k_perp = np.dot(delta_N, ni_k0)\n",
    "        \n",
    "        # Tangential component\n",
    "        c_i_k_tang = np.linalg.norm(delta_N - c_i_k_perp * ni_k0)\n",
    "\n",
    "        # Fill the feature vector for the point\n",
    "        features[i, :3] = N_i_k[i]         # Center of mass at scale k\n",
    "        features[i, 3:6] = N_i_k0[i]       # Center of mass at largest scale\n",
    "        features[i, 6:9] = delta_N         # Difference between centers of mass\n",
    "        features[i, 9] = c_i_k_perp        # Perpendicular distance\n",
    "        features[i, 10] = c_i_k_tang       # Tangential distance\n",
    "        features[i, 11] = distances[i].mean()  # Mean distance to neighbors\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_embedding(points_dict, k_scales=[2, 4, 8, 16]):\n",
    "\n",
    "    results = {}\n",
    "    \n",
    "    for file_name, data in points_dict.items():\n",
    "        print(f\"Processing file: {file_name}\")\n",
    "\n",
    "        points_3d = data.get('points_3d')\n",
    "        if points_3d is None:\n",
    "            print(f\"Error: Missing 'points_3d' data for {file_name}.\")\n",
    "            continue\n",
    "        \n",
    "        points_3d = np.array(points_3d)\n",
    "        multi_scale_features = []\n",
    "        \n",
    "        for k in k_scales:\n",
    "            scale_features = compute_scale_feature(points_3d, k)\n",
    "            multi_scale_features.append(scale_features)\n",
    "\n",
    "        # Concatenate features from all scales\n",
    "        X_i = np.hstack(multi_scale_features)\n",
    "        results[file_name] = X_i\n",
    "\n",
    "        print(f\"Completed processing for file: {file_name}\")\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "batch_size = 2\n",
    "k = 10\n",
    "radius = 0.70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 200]\n"
     ]
    }
   ],
   "source": [
    "def decimate_list(num_points_list):\n",
    "    return num_points_list[::2]\n",
    "\n",
    "# Sample num_points_list (you should replace this with your actual data)\n",
    "num_points_list = [100, 150, 200, 250]  # Example list of number of points\n",
    "num_points_list = decimate_list(num_points_list)\n",
    "\n",
    "print(num_points_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "points_batch = []\n",
    "features_batch = []\n",
    "labels_batch = []\n",
    "\n",
    "# Find maximum number of points in the batch\n",
    "max_num_points = max(num_points_list)\n",
    "\n",
    "for i in range(batch_size):\n",
    "    num_points = num_points_list[i]  # Get the number of points for this batch item\n",
    "    \n",
    "    # Generate random points (e.g., points on a sphere)\n",
    "    theta = np.random.uniform(0, 2 * np.pi, num_points)\n",
    "    phi = np.random.uniform(0, np.pi, num_points)\n",
    "    \n",
    "    x = radius * np.sin(phi) * np.cos(theta)\n",
    "    y = radius * np.sin(phi) * np.sin(theta)\n",
    "    z = radius * np.cos(phi)\n",
    "    points = np.stack([x, y, z], axis=1)  # (N, 3)\n",
    "    \n",
    "    # Compute scale features\n",
    "    features = compute_scale_feature(points, k = 10)\n",
    "    \n",
    "    # Generate synthetic labels (for demonstration, randomly assign edges)\n",
    "    labels = np.random.randint(0, 2, size=(num_points,))  # (N,)\n",
    "    \n",
    "    # Pad points, features, and labels\n",
    "    pad_size_points = max_num_points - num_points\n",
    "    points_padded = np.pad(points, ((0, pad_size_points), (0, 0)), mode='constant', constant_values=0)\n",
    "    \n",
    "    pad_size_features = max_num_points - features.shape[0]\n",
    "    features_padded = np.pad(features, ((0, pad_size_features), (0, 0)), mode='constant', constant_values=0)\n",
    "    \n",
    "    pad_size_labels = max_num_points - len(labels)\n",
    "    labels_padded = np.pad(labels, (0, pad_size_labels), mode='constant', constant_values=0)\n",
    "    \n",
    "    points_batch.append(points_padded)\n",
    "    features_batch.append(features_padded)\n",
    "    labels_batch.append(labels_padded)\n",
    "\n",
    "# Convert lists to numpy arrays for easier manipulation\n",
    "points_batch = np.array(points_batch)  # Shape: (batch_size, max_num_points, 3)\n",
    "features_batch = np.array(features_batch)  # Shape: (batch_size, max_num_points, 12)\n",
    "labels_batch = np.array(labels_batch)  # Shape: (batch_size, max_num_points)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PCEDNet(input_dim = 12, hidden_dim = 64, output_dim = 2, radius = 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_top_edges(edge_probabilities, top_k=10):\n",
    "\n",
    "    # We need to flatten the tensor to sort the edges\n",
    "    edge_probabilities = edge_probabilities.view(-1)  # Flatten to (B * N, output_dim)\n",
    "    \n",
    "    # Get the indices of the top_k probabilities\n",
    "    topk_values, topk_indices = torch.topk(edge_probabilities, top_k, largest = True)\n",
    "    \n",
    "    return topk_indices.tolist(), topk_values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data_and_extract_edges(points_dict, model, k=10, top_k=10):\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for file_name, data in points_dict.items():\n",
    "        points = data['points_3d']  # Get the 3D points from the dictionary\n",
    "\n",
    "        # Compute features\n",
    "        features = compute_scale_feature(points, k)\n",
    "\n",
    "        # Convert to PyTorch tensors\n",
    "        points_tensor = torch.tensor(points, dtype=torch.float).unsqueeze(0)  # (1, N, 3)\n",
    "        features_tensor = torch.tensor(features, dtype=torch.float).unsqueeze(0)  # (1, N, 12)\n",
    "\n",
    "        # Forward pass to obtain edge probabilities\n",
    "        edge_probabilities = model(points_tensor, features_tensor)  # (B, N, output_dim)\n",
    "\n",
    "        # Assume edge_probabilities shape is (B, N, N) where N is the number of points\n",
    "        edge_probabilities = edge_probabilities.squeeze(0)  # Remove batch dimension -> (N, N)\n",
    "        N = edge_probabilities.shape[0]\n",
    "\n",
    "        # Generate all possible point pairs (i, j) for the edges\n",
    "        edges = [(i, j) for i in range(N) for j in range(i+1, N)]\n",
    "\n",
    "        # Flatten the edge probabilities and map them to the corresponding point pairs\n",
    "        edge_probabilities_flat = edge_probabilities.view(-1)\n",
    "        \n",
    "        # Extract top_k edge probabilities and their indices\n",
    "        topk_values, topk_indices = torch.topk(edge_probabilities_flat, top_k, largest=True)\n",
    "\n",
    "        # Map the flattened indices back to the corresponding point pairs (edges)\n",
    "        top_edges = [edges[idx] for idx in topk_indices.tolist()]\n",
    "\n",
    "        # Print debugging information\n",
    "        print(f'File {file_name} Edge probability values {topk_values.tolist()}')\n",
    "        print(f'File {file_name} Top edges: {top_edges}')\n",
    "\n",
    "        # Store the results for the file, including the edges and probabilities\n",
    "        results.append({\n",
    "            'file_name': file_name,\n",
    "            'points': points.tolist(),  # Store the points for later use if needed\n",
    "            'top_edges': top_edges,  # Actual point pairs (edges)\n",
    "            'top_edges_probabilities': topk_values.tolist()  # Edge probabilities\n",
    "        })\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import cKDTree\n",
    "\n",
    "def radius_graph(points, radius):\n",
    "\n",
    "    # points_np = points.detach().cpu().numpy()  # Convert to NumPy array\n",
    "    points_np = points\n",
    "    tree = cKDTree(points_np)  # Create a KDTree for efficient radius queries\n",
    "\n",
    "    edge_index_list = []\n",
    "\n",
    "    for i in range(len(points_np)):\n",
    "        indices = tree.query_ball_point(points_np[i], radius)  # Find neighbors within the radius\n",
    "        for idx in indices:\n",
    "            if idx != i:  # Avoid self-loops\n",
    "                edge_index_list.append((i, idx))\n",
    "\n",
    "    # Convert edge_index_list to tensor\n",
    "    edge_index = torch.tensor(edge_index_list, dtype=torch.long).t().contiguous()\n",
    "    \n",
    "    return edge_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NEURAL NETWORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_indexes = radius_graph(points, radius)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def point_cloud_to_graph(point_cloud, num_neighbors=8):\n",
    "    \n",
    "    \"\"\" Convert point cloud to graph by finding k nearest neighbors for each point. \"\"\"\n",
    "    # Point cloud: (N, 3) -> list of 3D points\n",
    "    nbrs = NearestNeighbors(n_neighbors=num_neighbors, algorithm='ball_tree').fit(point_cloud)\n",
    "    edges = nbrs.kneighbors_graph(point_cloud).toarray()  # Adjacency matrix\n",
    "    \n",
    "    # Edge index list (2, E) with source and destination node indices for each edge\n",
    "    edge_index = torch.tensor(edges.nonzero(), dtype=torch.long)\n",
    "    \n",
    "    # Convert to PyTorch Geometric Data object\n",
    "    data = Data(x=torch.tensor(point_cloud, dtype=torch.float), edge_index=edge_index)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\oscar\\AppData\\Local\\Temp\\ipykernel_22468\\2467106094.py:9: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\b\\abs_6fueooay2f\\croot\\pytorch-select_1707342446212\\work\\torch\\csrc\\utils\\tensor_new.cpp:278.)\n",
      "  edge_index = torch.tensor(edges.nonzero(), dtype=torch.long)\n"
     ]
    }
   ],
   "source": [
    "list_of_graphs = []\n",
    "\n",
    "# Iterate through the dictionary and extract point cloud data\n",
    "for file_name, point_cloud in points_dict.items():\n",
    "    \n",
    "    # Extract 3D points from the point_cloud dictionary\n",
    "    point_cloud_data = point_cloud['points_3d']  # Use 'points_3d' key\n",
    "    point_cloud_data = np.array(point_cloud_data)\n",
    "    \n",
    "    # Ensure point_cloud_data is a 2D array (N, 3)\n",
    "    if point_cloud_data.shape[1] != 3:\n",
    "        raise ValueError(f\"Point cloud from {file_name} does not have 3 coordinates per point.\")\n",
    "    \n",
    "    # Convert point cloud to graph\n",
    "    graph_data = point_cloud_to_graph(point_cloud_data, num_neighbors=8)\n",
    "    list_of_graphs.append(graph_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import knn_graph, GCNConv\n",
    "\n",
    "class PCENet(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(PCENet, self).__init__()\n",
    "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
    "        self.conv3 = GCNConv(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, data):\n",
    "        # data contains point features (x) and edges (edge_index)\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.relu(self.conv2(x, edge_index))\n",
    "        x = self.conv3(x, edge_index)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EdgePredictor(torch.nn.Module):\n",
    "    def __init__(self, node_feature_dim):\n",
    "        super(EdgePredictor, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(node_feature_dim * 2, 128)\n",
    "        self.fc2 = torch.nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, node_i, node_j):\n",
    "        # Concatenate features of two nodes\n",
    "        edge_features = torch.cat([node_i, node_j], dim=-1)\n",
    "        edge_features = F.relu(self.fc1(edge_features))\n",
    "        edge_score = torch.sigmoid(self.fc2(edge_features))\n",
    "        \n",
    "        return edge_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OBJ GT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base folder:  c:\\Users\\oscar\\OneDrive - Fondazione Bruno Kessler\\Building_3D_Challenge\\data\\tallinn\n",
      "Folder obj:  c:\\Users\\oscar\\OneDrive - Fondazione Bruno Kessler\\Building_3D_Challenge\\data\\tallinn\\train\\wireframe\n",
      "Output folder obj: c:\\Users\\oscar\\OneDrive - Fondazione Bruno Kessler\\Building_3D_Challenge\\data\\tallinn\\outputs\n"
     ]
    }
   ],
   "source": [
    "base_folder_obj = Path(os.getcwd()).parents[0] / 'data' / 'tallinn'\n",
    "folder_obj = base_folder_obj /'train' / 'wireframe' \n",
    "output_folder_obj = base_folder_obj / 'outputs'\n",
    "\n",
    "print ('Base folder: ', base_folder_obj)\n",
    "print ('Folder obj: ', folder_obj)\n",
    "print ('Output folder obj:', output_folder_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of .obj files 7768 Type of file <class 'list'>\n"
     ]
    }
   ],
   "source": [
    "obj_files = [f for f in os.listdir(folder_obj) if f.endswith('.obj')]\n",
    "print(f'Number of .obj files {len(obj_files)} Type of file {type(obj_files)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_obj_file(filename):\n",
    "\n",
    "    vertices = []\n",
    "    edges = set()  # Use a set to avoid duplicate edges\n",
    "\n",
    "    with open(filename, 'r') as file:\n",
    "        for line in file:\n",
    "            parts = line.split()\n",
    "\n",
    "            if len(parts) == 0:\n",
    "                continue\n",
    "            \n",
    "            if parts[0] == 'v':  # Vertex\n",
    "                try:\n",
    "                    # Ensure there are at least 3 components for the vertex coordinates\n",
    "                    vertices.append(list(map(float, parts[1:4])))\n",
    "                except ValueError:\n",
    "                    print(f\"Warning: Invalid vertex data in line: {line.strip()}\")\n",
    "\n",
    "            elif parts[0] == 'l':  # Line\n",
    "                try:\n",
    "                    line_indices = [int(p) - 1 for p in parts[1:]]\n",
    "                    if len(line_indices) == 2:\n",
    "                        edge = (min(line_indices[0], line_indices[1]), max(line_indices[0], line_indices[1]))\n",
    "                        edges.add(edge)\n",
    "                    else:\n",
    "                        print(f\"Warning: Line with incorrect number of indices in line: {line.strip()}\")\n",
    "                except (ValueError, IndexError):\n",
    "                    print(f\"Warning: Invalid line data in line: {line.strip()}\")\n",
    "\n",
    "    vertices = np.array(vertices)\n",
    "    edges = list(edges)  # Convert the set of edges to a list before returning\n",
    "\n",
    "    return vertices, edges\n",
    "\n",
    "# Directory containing .obj files\n",
    "folder_obj = Path(os.getcwd()).parents[0] / 'data' / 'tallinn' / 'train' / 'wireframe'\n",
    "\n",
    "# List all .obj files in the directory\n",
    "obj_files = [f for f in os.listdir(folder_obj) if f.endswith('.obj')]\n",
    "\n",
    "# Dictionary to store vertices and edges for each .obj file\n",
    "obj_data = {}\n",
    "\n",
    "for obj_file in obj_files:\n",
    "    file_obj_path = os.path.join(folder_obj, obj_file)\n",
    "    print(f\"Parsing file: {file_obj_path}\")\n",
    "    \n",
    "    # Parse the .obj file and get vertices and edges\n",
    "    vertices, edges = parse_obj_file(file_obj_path)\n",
    "    \n",
    "    # Ensure vertices is a NumPy array\n",
    "    vertices = np.array(vertices)  # Convert vertices to a NumPy array if not already\n",
    "    \n",
    "    if not isinstance(edges, list):\n",
    "        print(f\"Error: edges for {obj_file} is not a list.\")\n",
    "        continue\n",
    "    \n",
    "    # Convert edge indices to a list of tuples if not already\n",
    "    edge_index = [(int(edge[0]), int(edge[1])) for edge in edges]\n",
    "    \n",
    "    # Check if vertices has the correct shape\n",
    "    if len(vertices.shape) != 2 or vertices.shape[1] != 3:\n",
    "        print(f\"Error: vertices for {obj_file} is not a 2D array with (n, 3) shape.\")\n",
    "        continue\n",
    "    \n",
    "    # Check if edges is a list of index pairs\n",
    "    if not all(isinstance(edge, (list, tuple)) and len(edge) == 2 for edge in edges):\n",
    "        print(f\"Error: edges for {obj_file} contains invalid elements.\")\n",
    "        continue\n",
    "    \n",
    "    # Generate edge_coordinates from vertices\n",
    "    edge_coordinates = [(vertices[edge[0]], vertices[edge[1]]) for edge in edges]\n",
    "\n",
    "    # Store vertices, edge index (indices), and edge coordinates in the dictionary\n",
    "    obj_data[obj_file] = {\n",
    "        'vertices': vertices.tolist(),  # Convert NumPy array to list for storage\n",
    "        'edge_index': edge_index,  # Original index pairs\n",
    "        'edges': edge_coordinates  # Coordinates of edges (pair of vertex coordinates)\n",
    "    }\n",
    "    \n",
    "    # Output information for verification\n",
    "    print(f\"File: {obj_file}\")\n",
    "    print(f\"Vertices shape: {vertices.shape}, Number of edges: {len(edges)}\")\n",
    "    print(f\"Sample edge indices (first 5): {edge_index[:5]}\")\n",
    "    print(f\"Sample edge coordinates (first 5): {edge_coordinates[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ground_truth_tensor(vertices_index, edges):\n",
    "\n",
    "    ground_truth_tensor = []\n",
    "\n",
    "    for idx, edge in enumerate(vertices_index):\n",
    "        edge_data = {\n",
    "            'index': edge,  # This is the pair of vertex indices\n",
    "            'coordinates': edges[idx]  # The corresponding coordinates of the edge\n",
    "        }\n",
    "        ground_truth_tensor.append(edge_data)\n",
    "\n",
    "    return ground_truth_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and store the ground truth tensors and edges\n",
    "ground_truth_tensors = []\n",
    "ground_truth_edges = []\n",
    "\n",
    "for obj_file, data in obj_data.items():\n",
    "    vertices_index = data['edge_index']\n",
    "    edges = data['edges']\n",
    "    \n",
    "    # Create ground truth tensor\n",
    "    ground_truth_tensor = create_ground_truth_tensor(vertices_index, edges)\n",
    "    ground_truth_tensors.append({\n",
    "        'file': obj_file,\n",
    "        'tensor': ground_truth_tensor\n",
    "    })\n",
    "    \n",
    "    # Create ground truth edges\n",
    "    ground_truth_edge_data = [{'index': index, 'coordinates': coordinates} for index, coordinates in zip(vertices_index, edges)]\n",
    "    ground_truth_edges.append({\n",
    "        'file': obj_file,\n",
    "        'edges': ground_truth_edge_data\n",
    "    })\n",
    "    \n",
    "    print(f\"Ground Truth Tensor for {obj_file}:\")\n",
    "    print(ground_truth_tensor)\n",
    "\n",
    "    print(f\"Ground Truth Edges for {obj_file}:\")\n",
    "    print(ground_truth_edge_data)\n",
    "\n",
    "# Optionally, you can print the full list of ground truth tensors and edges\n",
    "print(\"All Ground Truth Tensors:\")\n",
    "print(ground_truth_tensors)\n",
    "\n",
    "print(\"All Ground Truth Edges:\")\n",
    "print(ground_truth_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "\n",
    "# Example of how to create a Data object\n",
    "# Replace with your actual data loading process\n",
    "data_list = []\n",
    "for obj_file, data in obj_data.items():\n",
    "    # Create edge_index tensor\n",
    "    edge_index = torch.tensor(data['edge_index'], dtype=torch.long).t().contiguous()\n",
    "    \n",
    "    # Create node features tensor\n",
    "    x = torch.tensor(data['vertices'], dtype=torch.float)\n",
    "    \n",
    "    # Create a Data object\n",
    "    graph_data = Data(x=x, edge_index=edge_index)\n",
    "    data_list.append(graph_data)\n",
    "\n",
    "# DataLoader for point cloud graphs\n",
    "data_loader = DataLoader(data_list, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def prepare_ground_truth_edges(ground_truth_edges):\n",
    "    # Flatten the list of edge data into a format suitable for loss calculation\n",
    "    edge_indices = []\n",
    "    edge_labels = []\n",
    "    \n",
    "    for entry in ground_truth_edges:\n",
    "        for edge_data in entry['edges']:\n",
    "            edge_indices.append(edge_data['index'])\n",
    "            edge_labels.append(1)  # Assuming all provided edges are positive examples\n",
    "\n",
    "    # Convert to tensors\n",
    "    edge_indices = torch.tensor(edge_indices, dtype=torch.long)\n",
    "    edge_labels = torch.tensor(edge_labels, dtype=torch.float)\n",
    "\n",
    "    return edge_indices, edge_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch_geometric\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "class PCENet(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(PCENet, self).__init__()\n",
    "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
    "        self.conv3 = GCNConv(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, data):\n",
    "        # Ensure data is a torch_geometric.data.Data object\n",
    "        if not isinstance(data, torch_geometric.data.Data):\n",
    "            raise TypeError(\"Input to the model must be a torch_geometric.data.Data object.\")\n",
    "\n",
    "        # Extract node features and edge index from data\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        \n",
    "        # Forward pass through the network\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.relu(self.conv2(x, edge_index))\n",
    "        x = self.conv3(x, edge_index)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_indices, ground_truth_edge_labels = prepare_ground_truth_edges(ground_truth_edges)\n",
    "\n",
    "# Instantiate models\n",
    "pc_net = PCENet(input_dim=3, hidden_dim=64, output_dim=64)  # Assume 3D point cloud as input\n",
    "edge_predictor = EdgePredictor(node_feature_dim=64)\n",
    "\n",
    "# Optimizer and loss function\n",
    "optimizer = optim.Adam(list(pc_net.parameters()) + list(edge_predictor.parameters()), lr=0.001)\n",
    "criterion = torch.nn.BCELoss()\n",
    "\n",
    "# DataLoader for point cloud graphs (assuming you have a list of Data objects)\n",
    "data_loader = DataLoader(list_of_graphs, batch_size=1, shuffle=True)  # list_of_graphs contains graph Data objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_hull_polygons(points_3d):\n",
    "    # Ensure points_3d is a numpy array\n",
    "    points_3d = np.array(points_3d)\n",
    "    \n",
    "    # Compute the convex hull\n",
    "    hull = ConvexHull(points_3d)\n",
    "    hull_polygons = []\n",
    "\n",
    "    # Extract vertices of each face of the convex hull\n",
    "    for simplex in hull.simplices:\n",
    "        # Ensure the polygon is closed by repeating the first vertex\n",
    "        simplex_closed = np.append(simplex, simplex[0])\n",
    "        polygon = points_3d[simplex_closed]\n",
    "        hull_polygons.append(polygon)\n",
    "    \n",
    "    return hull_polygons, hull\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "hull_polygons, hull = compute_hull_polygons(points)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "COMPUTE EDGES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d.art3d import Line3DCollection\n",
    "\n",
    "def plot_closed_polyline(ax, closed_polyline):\n",
    "    # Create a list of edges from the ordered vertices\n",
    "    polyline_edges = [[closed_polyline[i], closed_polyline[i+1]] for i in range(len(closed_polyline) - 1)]\n",
    "    \n",
    "    # Create the Line3DCollection for the polyline\n",
    "    polyline_collection = Line3DCollection(polyline_edges, colors='g', linewidths=2, linestyle='--')\n",
    "    \n",
    "    # Add the polyline to the 3D plot\n",
    "    ax.add_collection3d(polyline_collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_perimeter_edges(points_3d, hull, k, max_edges=33):\n",
    "    features = compute_scale_feature(points_3d, k)\n",
    "    \n",
    "    perimeter_edges = set()\n",
    "    \n",
    "    for simplex in hull.simplices:\n",
    "        for i in range(len(simplex)):\n",
    "            edge = (simplex[i], simplex[(i + 1) % len(simplex)])\n",
    "            edge = tuple(sorted(edge))\n",
    "            perimeter_edges.add(edge)\n",
    "    \n",
    "    edge_scores = []\n",
    "\n",
    "    for edge in perimeter_edges:\n",
    "        p1, p2 = edge\n",
    "        \n",
    "        if not (0 <= p1 < len(points_3d) and 0 <= p2 < len(points_3d)):\n",
    "            continue\n",
    "\n",
    "        closest_points = np.vstack([points_3d[p1], points_3d[p2]])\n",
    "        n = points_3d[p2] - points_3d[p1]\n",
    "        n = n / np.linalg.norm(n)\n",
    "\n",
    "        try:\n",
    "            N_i_upper_edge, N_i_lower_edge, Sigma_i_upper_edge, Sigma_i_lower_edge, N_i_upper_center_edge, N_i_lower_center_edge = partition_and_compute_covariance(closest_points, n)\n",
    "            N_i_upper_full, N_i_lower_full, Sigma_i_upper_full, Sigma_i_lower_full, N_i_upper_center_full, N_i_lower_center_full = partition_and_compute_covariance(points_3d, n)\n",
    "        except Exception as e:\n",
    "            print(f\"Error in partition_and_compute_covariance for edge {edge}: {e}\")\n",
    "            continue\n",
    "\n",
    "        Sigma_i_upper = np.mean([Sigma_i_upper_edge, Sigma_i_upper_full], axis=0) if Sigma_i_upper_edge is not None and Sigma_i_upper_full is not None else None\n",
    "        Sigma_i_lower = np.mean([Sigma_i_lower_edge, Sigma_i_lower_full], axis=0) if Sigma_i_lower_edge is not None and Sigma_i_lower_full is not None else None\n",
    "        N_i_upper_center = np.mean([N_i_upper_center_edge, N_i_upper_center_full], axis=0) if N_i_upper_center_edge is not None and N_i_upper_center_full is not None else None\n",
    "        N_i_lower_center = np.mean([N_i_lower_center_edge, N_i_lower_center_full], axis=0) if N_i_lower_center_edge is not None and N_i_lower_center_full is not None else None\n",
    "\n",
    "        dist_upper_to_edge = np.linalg.norm(N_i_upper_center - np.mean(closest_points, axis=0)) if N_i_upper_center is not None else 0\n",
    "        dist_lower_to_edge = np.linalg.norm(N_i_lower_center - np.mean(closest_points, axis=0)) if N_i_lower_center is not None else 0\n",
    "\n",
    "        eigenvalue_ratio_upper = Sigma_i_upper[0] / Sigma_i_upper[-1] if Sigma_i_upper is not None and Sigma_i_upper[-1] > 0 else 0\n",
    "        eigenvalue_ratio_lower = Sigma_i_lower[0] / Sigma_i_lower[-1] if Sigma_i_lower is not None and Sigma_i_lower[-1] > 0 else 0\n",
    "\n",
    "        point_density = len(closest_points) / np.linalg.norm(points_3d[p1] - points_3d[p2])\n",
    "\n",
    "        alpha, beta, gamma = 0.8, 0.15, 0.05\n",
    "        score = (alpha * (eigenvalue_ratio_upper + eigenvalue_ratio_lower) \n",
    "                - beta * (dist_upper_to_edge + dist_lower_to_edge) \n",
    "                + gamma * point_density)\n",
    "        \n",
    "        edge_scores.append((score, edge))\n",
    "    \n",
    "    edge_scores.sort(key=lambda x: x[0], reverse=True)\n",
    "    top_edges = [edge for score, edge in edge_scores[:max_edges]]\n",
    "    \n",
    "    return top_edges\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_noise(points_3d, radius=0.05, min_neighbors=5):\n",
    "    \"\"\"\n",
    "    Remove noise from a point cloud based on a distance threshold.\n",
    "\n",
    "    Parameters:\n",
    "    - points_3d: Array of shape (n_points, 3) containing the 3D coordinates of the points.\n",
    "    - radius: Radius for neighbor search (default: 0.05).\n",
    "    - min_neighbors: Minimum number of neighbors required for a point to be considered non-noise (default: 5).\n",
    "\n",
    "    Returns:\n",
    "    - Array of shape (n_filtered_points, 3) with noise points removed.\n",
    "    \"\"\"\n",
    "    # Initialize NearestNeighbors\n",
    "    nbrs = NearestNeighbors(n_neighbors=min_neighbors, radius=radius).fit(points_3d)\n",
    "    \n",
    "    # Find neighbors\n",
    "    distances, indices = nbrs.radius_neighbors(points_3d)\n",
    "    \n",
    "    # Count the number of neighbors for each point\n",
    "    num_neighbors = np.array([len(idx) for idx in indices])\n",
    "    \n",
    "    # Filter points: keep points with at least min_neighbors\n",
    "    filtered_points = points_3d[num_neighbors >= min_neighbors]\n",
    "    \n",
    "    return filtered_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_point_cloud_and_hull_3figures(file_name, points_3d, hull_polygons, perimeter_edges_1, top_edges):\n",
    "\n",
    "    fig = plt.figure(figsize=(18, 15))\n",
    "\n",
    "    # Define subplots\n",
    "    ax1 = fig.add_subplot(131, projection='3d')\n",
    "    ax2 = fig.add_subplot(132, projection='3d')\n",
    "    ax3 = fig.add_subplot(133, projection='3d')\n",
    "\n",
    "    # Subplot 1: Point cloud only (no hull)\n",
    "    ax1.scatter(points_3d[:, 0], points_3d[:, 1], points_3d[:, 2], color='r', marker='o', s=0.50, label='Point cloud')\n",
    "    ax1.set_title(f'POINT CLOUD FOR {file_name}', fontsize=10)\n",
    "    ax1.set_xlabel('x')\n",
    "    ax1.set_ylabel('y')\n",
    "    ax1.set_zlabel('z')\n",
    "    ax1.xaxis.set_major_formatter(ticker.FormatStrFormatter('%.2f'))\n",
    "    ax1.yaxis.set_major_formatter(ticker.FormatStrFormatter('%.2f'))\n",
    "    ax1.zaxis.set_major_formatter(ticker.FormatStrFormatter('%.2f'))\n",
    "    ax1.tick_params(axis='both', which='major', labelsize=6)\n",
    "    ax1.tick_params(axis='both', which='minor', labelsize=6)\n",
    "\n",
    "    # Subplot 2: Point cloud and convex hull polygons (from hull_1, hull_polygons_1)\n",
    "    ax2.scatter(points_3d[:, 0], points_3d[:, 1], points_3d[:, 2], color='r', marker='o', s=0.50, label='Point cloud')\n",
    "\n",
    "    for polygon in hull_polygons_1:\n",
    "        poly = Poly3DCollection([polygon], alpha=0.10, linewidths=0.50, edgecolors='b')\n",
    "        ax2.add_collection3d(poly)\n",
    "        \n",
    "    ax2.set_title(f'POINT CLOUD AND CONVEX HULL (ORIGINAL) FOR {file_name}', fontsize=8)\n",
    "    ax2.set_xlabel('x')\n",
    "    ax2.set_ylabel('y')\n",
    "    ax2.set_zlabel('z')\n",
    "    ax2.xaxis.set_major_formatter(ticker.FormatStrFormatter('%.2f'))\n",
    "    ax2.yaxis.set_major_formatter(ticker.FormatStrFormatter('%.2f'))\n",
    "    ax2.zaxis.set_major_formatter(ticker.FormatStrFormatter('%.2f'))\n",
    "    ax2.tick_params(axis='both', which='major', labelsize=6)\n",
    "    ax2.tick_params(axis='both', which='minor', labelsize=6)\n",
    "\n",
    "    # Subplot 3: Filtered point cloud, convex hull polygons (hull_2), and perimeter edges (perimeter_edges_2)\n",
    "    ax3.scatter(filtered_points[:, 0], filtered_points[:, 1], filtered_points[:, 2], color='r', marker='o', s=0.50, label='Filtered point cloud')\n",
    "\n",
    "    # Plot convex hull polygons (from hull_polygons_2)\n",
    "    for polygon in hull_polygons_2:\n",
    "        poly = Poly3DCollection([polygon], alpha=0.10, linewidths=0.50, edgecolors='g', linestyle='--')\n",
    "        ax3.add_collection3d(poly)\n",
    "\n",
    "    # Compute the convex hull of the filtered points\n",
    "    hull = ConvexHull(filtered_points)\n",
    "\n",
    "    # Extract edges (simplices) from the convex hull, which represent the boundary edges\n",
    "    hull_edges = hull.simplices  # simplices are pairs of indices that form edges\n",
    "\n",
    "    # Select up to 10 extreme (border) edges from the convex hull\n",
    "    edges_to_plot_perimeter = [\n",
    "        [filtered_points[edge[0]], filtered_points[edge[1]]]\n",
    "        for edge in hull_edges[:20]  # Select up to 10 edges from the convex hull\n",
    "    ]\n",
    "\n",
    "    # Plot the selected edges\n",
    "    if edges_to_plot_perimeter:\n",
    "        edge_collection_perimeter = Line3DCollection(edges_to_plot_perimeter, colors='r', linewidths=2, alpha=1.0, label='Perimeter Edges')\n",
    "        ax3.add_collection3d(edge_collection_perimeter)\n",
    "\n",
    "    # Plot the selected edges\n",
    "    if edges_to_plot_perimeter:\n",
    "        edge_collection_perimeter = Line3DCollection(edges_to_plot_perimeter, colors='r', linewidths=2, alpha=1.0, label='Perimeter Edges')\n",
    "        ax3.add_collection3d(edge_collection_perimeter)\n",
    "\n",
    "    ax3.set_title(f'FILTERED POINT CLOUD, HULL, PERIMETER EDGES, AND TOP EDGES FOR {file_name}', fontsize=10)\n",
    "    ax3.set_xlabel('x')\n",
    "    ax3.set_ylabel('y')\n",
    "    ax3.set_zlabel('z')\n",
    "    ax3.xaxis.set_major_formatter(ticker.FormatStrFormatter('%.2f'))\n",
    "    ax3.yaxis.set_major_formatter(ticker.FormatStrFormatter('%.2f'))\n",
    "    ax3.zaxis.set_major_formatter(ticker.FormatStrFormatter('%.2f'))\n",
    "    ax3.tick_params(axis='both', which='major', labelsize=6)\n",
    "    ax3.tick_params(axis='both', which='minor', labelsize=6)\n",
    "\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cvpr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
